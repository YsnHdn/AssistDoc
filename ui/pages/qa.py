"""
Page de questions-r√©ponses de l'application AssistDoc.
Permet de poser des questions sur les documents et affiche les r√©ponses avec sources.
"""

import streamlit as st
import time
import os
from pathlib import Path

# Import des modules de l'application
from ui.components.visualization import display_chat_message, display_model_info
from src.vector_db.retriever import create_default_retriever
from src.llm.models import LLMConfig, LLMProvider, create_llm

def detect_streamlit_cloud():
    """D√©tecte si l'application s'ex√©cute sur Streamlit Cloud"""
    return os.path.exists("/mount/src")

def show_qa_page():
    """
    Affiche la page de questions-r√©ponses avec un chat d√©di√© par document.
    """
    # Titre de la page
    st.markdown("<h1 class='main-title'>Questions & R√©ponses üí¨</h1>", unsafe_allow_html=True)
    
    # V√©rifier si des documents sont charg√©s
    if not st.session_state.get("documents", []) or not st.session_state.get("vector_store_initialized", False):
        st.warning("Aucun document charg√©. Veuillez d'abord charger et indexer des documents dans la barre lat√©rale.")
        return
    
    # Initialiser les historiques de chat par document s'ils n'existent pas
    if "document_chats" not in st.session_state:
        st.session_state.document_chats = {}
    
    # S√©lection du document pour la conversation
    doc_options = [doc.get("file_name", f"Document {i+1}") for i, doc in enumerate(st.session_state.documents)]
    
    selected_doc = st.selectbox(
        "S√©lectionnez un document pour discuter",
        options=doc_options
    )
    
    # Obtenir l'index du document s√©lectionn√©
    selected_doc_index = doc_options.index(selected_doc)
    selected_doc_id = st.session_state.documents[selected_doc_index].get("file_name")
    
    # Cr√©er un identifiant unique pour chaque document
    if selected_doc_id not in st.session_state.document_chats:
        st.session_state.document_chats[selected_doc_id] = []
    
    # Afficher la session de chat pour le document s√©lectionn√©
    st.subheader(f"Conversation sur: {selected_doc}")
    
    # Informations sur le mod√®le utilis√©
    display_model_info()
    
    # Section de chat pour le document s√©lectionn√©
    with st.container():
        # Afficher l'historique de chat pour ce document
        for message in st.session_state.document_chats[selected_doc_id]:
            display_chat_message(
                message["content"],
                is_user=message["role"] == "user"
            )
        
        # Zone de saisie pour la question
        user_question = st.chat_input(f"Posez une question sur {selected_doc}...")
        
        # Si l'utilisateur a pos√© une question
        if user_question:
            # Ajouter la question √† l'historique du document actuel
            st.session_state.document_chats[selected_doc_id].append({
                "role": "user",
                "content": user_question
            })
            
            # Afficher la question (mise √† jour imm√©diate de l'interface)
            display_chat_message(user_question, is_user=True)
            
            # Traiter la question et g√©n√©rer une r√©ponse
            with st.spinner(f"Recherche de la r√©ponse dans {selected_doc}..."):
                answer, sources = process_question(user_question, selected_doc_id, selected_doc_index)
            
            # Cr√©er le message de r√©ponse avec les sources
            response_message = {
                "role": "assistant",
                "content": answer,
                "sources": sources
            }
            
            # Ajouter la r√©ponse √† l'historique du document actuel
            st.session_state.document_chats[selected_doc_id].append({
                "role": "assistant",
                "content": response_message
            })
            
            # Afficher la r√©ponse
            display_chat_message(response_message)
    
    # Boutons d'action
    col1, col2 = st.columns(2)
    
    with col1:
        # Bouton pour effacer la conversation actuelle
        if st.session_state.document_chats.get(selected_doc_id) and st.button("Effacer cette conversation", type="secondary"):
            st.session_state.document_chats[selected_doc_id] = []
            st.rerun()
    
    with col2:
        # Bouton pour effacer toutes les conversations
        if any(st.session_state.document_chats.values()) and st.button("Effacer toutes les conversations", type="secondary"):
            st.session_state.document_chats = {key: [] for key in st.session_state.document_chats.keys()}
            st.rerun()

def process_question(question, doc_id, doc_index):
    """
    Traite une question et g√©n√®re une r√©ponse en utilisant le syst√®me RAG,
    en limitant la recherche au document sp√©cifi√©.
    
    Args:
        question: Question pos√©e par l'utilisateur
        doc_id: Identifiant du document
        doc_index: Index du document dans la liste des documents
        
    Returns:
        Tuple contenant (r√©ponse, sources)
    """
    try:
        # R√©cup√©rer les param√®tres de configuration LLM
        provider = st.session_state.llm_provider
        model = st.session_state.llm_model
        
        # Importer les cl√©s API depuis le fichier de configuration
        try:
            from config import API_KEYS, API_BASE_URLS
            api_key = API_KEYS.get(provider)
            api_base = API_BASE_URLS.get(provider)
        except ImportError:
            # Utiliser des valeurs par d√©faut si le fichier n'existe pas
            api_key = None
            api_base = "https://models.inference.ai.azure.com" if provider == "github_inference" else None
            st.warning("Fichier config.py non trouv√©. Les API n√©cessitant une authentification pourraient ne pas fonctionner.")
        
        # Cr√©er la configuration LLM
        config = LLMConfig(
            provider=provider,
            model_name=model,
            api_key=api_key,
            api_base=api_base,
            temperature=0.7,
            max_tokens=1000
        )
        
        # Si c'est Hugging Face, ajouter des param√®tres sp√©cifiques
        if provider == "huggingface":
            config.extra_params["use_api"] = True
        
        # Cr√©er le LLM
        llm = create_llm(config)
        
        # Cr√©er le retriever - Toujours utiliser FAISS sur Streamlit Cloud
        vector_store_path = "data/vector_store"
        store_type = "faiss"  # Toujours utiliser FAISS pour garantir la compatibilit√©
        retriever = create_default_retriever(
            store_path=vector_store_path,
            embedder_model="all-MiniLM-L6-v2",
            store_type=store_type,
            top_k=5
        )
        
        # APPROCHE AM√âLIOR√âE : R√©cup√©rer les chunks en deux √©tapes
        # 1. Essayer de r√©cup√©rer des chunks pertinents pour la question sp√©cifique
        question_chunks = retriever.retrieve(question, top_k=10)
        
        # Filtrer pour ne garder que les chunks du document s√©lectionn√©
        filtered_chunks = [chunk for chunk in question_chunks if chunk.get("file_name") == doc_id]
        
        # 2. Si pas assez de chunks pertinents trouv√©s, r√©cup√©rer des chunks g√©n√©raux du document
        if len(filtered_chunks) < 2:
            # Requ√™te g√©n√©rale sur le document
            doc_chunks = retriever.retrieve(
                f"contenu important de {doc_id}", 
                top_k=10
            )
            
            # Filtrer pour ce document sp√©cifique
            doc_chunks = [chunk for chunk in doc_chunks if chunk.get("file_name") == doc_id]
            
            # Si toujours pas de chunks, essayer d'acc√©der au texte brut du document
            if not doc_chunks and doc_index is not None:
                # R√©cup√©rer le document complet
                doc = st.session_state.documents[doc_index]
                if "full_text" in doc:
                    # Diviser le texte en petits morceaux
                    text = doc["full_text"]
                    chunks = []
                    chunk_size = 1000  # Taille approximative de chunk
                    
                    for i in range(0, len(text), chunk_size):
                        end = min(i + chunk_size, len(text))
                        chunk_text = text[i:end]
                        chunks.append({
                            "text": chunk_text,
                            "file_name": doc_id,
                            "score": 0.5  # Score arbitraire
                        })
                    
                    # Utiliser ces chunks comme fallback
                    if chunks:
                        doc_chunks = chunks[:5]  # Limiter √† 5 chunks
            
            # Ajouter les chunks du document √† ceux d√©j√† trouv√©s
            filtered_chunks.extend(doc_chunks)
            
            # √âliminer les doublons potentiels
            seen_texts = set()
            unique_chunks = []
            for chunk in filtered_chunks:
                text = chunk.get("text", "")
                if text not in seen_texts:
                    seen_texts.add(text)
                    unique_chunks.append(chunk)
            
            filtered_chunks = unique_chunks[:5]  # Limiter √† 5 chunks
        
        # S'il n'y a toujours pas de chunks, indiquer qu'il n'y a pas d'informations
        if not filtered_chunks:
            return f"Je n'ai pas trouv√© d'informations suffisantes dans le document {doc_id}. Ce document pourrait √™tre vide ou mal index√©.", []
        
        # Convertir les chunks en texte format√©
        context_text = ""
        for i, chunk in enumerate(filtered_chunks):
            context_text += f"[EXTRAIT {i+1}]\n{chunk.get('text', '')}\n\n"
        
        # Syst√®me prompt explicite
        system_prompt = """Tu es un assistant intelligent qui r√©pond aux questions sur des documents.
        R√©ponds en te basant sur les extraits fournis, m√™me si la r√©ponse n'est pas explicite.
        Si la question est g√©n√©rale (comme "de quoi parle ce document"), fais une synth√®se des extraits.
        N'invente pas d'informations qui ne seraient pas dans les extraits."""
        
        # Prompt utilisateur direct
        user_prompt = f"""Voici des extraits du document "{doc_id}":

{context_text}

Question: {question}

R√©ponds √† cette question en te basant sur les extraits ci-dessus. Si la question est g√©n√©rale (comme "de quoi parle ce document"), fais une synth√®se des informations pr√©sentes dans les extraits."""
        
        # G√©n√©rer directement la r√©ponse
        response = llm.generate(user_prompt, system_prompt=system_prompt)
        
        # Cr√©er les sources
        sources = []
        for chunk in filtered_chunks[:5]:  # Limiter √† 5 sources pour l'affichage
            source = {
                "text": chunk.get("text", ""),
                "file_name": chunk.get("file_name", "Document inconnu"),
                "score": chunk.get("score", 0)
            }
            sources.append(source)
        
        return response.content, sources
        
    except Exception as e:
        import traceback
        error_text = f"D√©sol√©, une erreur s'est produite: {str(e)}\n\n{traceback.format_exc()}"
        st.error(error_text)
        return f"D√©sol√©, une erreur s'est produite: {str(e)}", []