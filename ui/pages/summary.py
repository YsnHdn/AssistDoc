"""
Page de r√©sum√© de l'application AssistDoc.
Permet de g√©n√©rer des r√©sum√©s des documents avec diff√©rentes options.
"""

import streamlit as st
import time
import os
from pathlib import Path
import io

# Import des modules de l'application
from ui.components.visualization import display_summary, display_model_info
from src.vector_db.retriever import create_default_retriever
from src.llm.models import LLMConfig, LLMProvider, create_llm
from utils.session_utils import get_user_id, get_user_data_path, ensure_user_directories

def detect_streamlit_cloud():
    """D√©tecte si l'application s'ex√©cute sur Streamlit Cloud"""
    return os.path.exists("/mount/src")

def show_summary_page():
    """
    Affiche la page de g√©n√©ration de r√©sum√©.
    """
    # Titre de la page
    st.markdown("<h1 class='main-title'>R√©sum√© de Documents üìù</h1>", unsafe_allow_html=True)
    
    # V√©rifier si des documents sont charg√©s
    if not st.session_state.get("documents", []) or not st.session_state.get("vector_store_initialized", False):
        st.warning("Aucun document charg√©. Veuillez d'abord charger et indexer des documents dans la barre lat√©rale.")
        return
    
    # Informations sur le mod√®le utilis√©
    display_model_info()
    
    # Panneau de configuration du r√©sum√©
    with st.expander("Options de r√©sum√©", expanded=True):
        # S√©lection du document √† r√©sumer
        doc_options = [doc.get("file_name", f"Document {i+1}") for i, doc in enumerate(st.session_state.documents)]
        
        selected_doc = st.selectbox(
            "Document √† r√©sumer",
            options=doc_options
        )
        
        # Obtenir l'index du document s√©lectionn√©
        selected_doc_index = doc_options.index(selected_doc)
        selected_doc_id = st.session_state.documents[selected_doc_index].get("file_name")
        
        # Longueur du r√©sum√©
        col1, col2 = st.columns(2)
        
        with col1:
            length_options = {
                "court": "Court (environ 150 mots)",
                "moyen": "Moyen (environ 300 mots)",
                "long": "Long (environ 500 mots)",
                "personnalis√©": "Personnalis√©"
            }
            
            summary_length = st.selectbox(
                "Longueur du r√©sum√©",
                options=list(length_options.keys()),
                format_func=lambda x: length_options[x],
                index=1  # Option par d√©faut: moyen
            )
            
            # Si longueur personnalis√©e
            if summary_length == "personnalis√©":
                custom_length = st.number_input("Nombre de mots", min_value=50, max_value=1000, value=300, step=50)
                summary_length_value = f"environ {custom_length} mots"
            else:
                # Convertir l'option en valeur utilisable
                summary_length_map = {
                    "court": "environ 150 mots",
                    "moyen": "environ 300 mots",
                    "long": "environ 500 mots"
                }
                summary_length_value = summary_length_map.get(summary_length, "environ 300 mots")
        
        with col2:
            # Style du r√©sum√©
            style_options = {
                "informatif": "Informatif (factuel)",
                "analytique": "Analytique (critique)",
                "simplifi√©": "Simplifi√© (vulgaris√©)",
                "bullet_points": "Points cl√©s (liste)"
            }
            
            summary_style = st.selectbox(
                "Style du r√©sum√©",
                options=list(style_options.keys()),
                format_func=lambda x: style_options[x],
                index=0  # Option par d√©faut: informatif
            )
        
        # S√©lection des documents √† r√©sumer (si plusieurs sont charg√©s)
        if len(st.session_state.documents) > 1:
            multi_doc = st.checkbox("Inclure d'autres documents dans le r√©sum√©", value=False)
            
            if multi_doc:
                additional_docs = st.multiselect(
                    "Documents additionnels",
                    options=[doc for doc in doc_options if doc != selected_doc],
                    default=[],
                    help="S√©lectionnez des documents suppl√©mentaires √† inclure dans le r√©sum√©"
                )
                
                # Ajouter le document principal aux documents s√©lectionn√©s
                selected_docs = [selected_doc] + additional_docs
                
                # Filtrer les documents s√©lectionn√©s
                selected_doc_indices = [i for i, name in enumerate(doc_options) if name in selected_docs]
            else:
                # Si un seul document est s√©lectionn√©
                selected_doc_indices = [selected_doc_index]
        else:
            # Si un seul document est charg√©, le s√©lectionner automatiquement
            selected_doc_indices = [0]
    
    # Bouton pour g√©n√©rer le r√©sum√©
    if st.button("G√©n√©rer le r√©sum√©", type="primary", use_container_width=True):
        # V√©rifier qu'au moins un document est s√©lectionn√©
        if not selected_doc_indices:
            st.error("Veuillez s√©lectionner au moins un document √† r√©sumer")
            return
        
        # G√©n√©rer le r√©sum√©
        with st.spinner("G√©n√©ration du r√©sum√© en cours..."):
            summary_text, metadata = generate_summary(
                summary_length_value,
                summary_style,
                selected_doc_indices
            )
        
        # Afficher le r√©sum√©
        if summary_text:
            display_summary(summary_text, metadata)
            
            # Permettre le t√©l√©chargement du r√©sum√©
            offer_download(summary_text, metadata)

def generate_summary(length, style, doc_indices):
    """
    G√©n√®re un r√©sum√© des documents s√©lectionn√©s.
    
    Args:
        length: Longueur souhait√©e pour le r√©sum√©
        style: Style souhait√© pour le r√©sum√©
        doc_indices: Indices des documents √† r√©sumer
        
    Returns:
        Tuple contenant (texte du r√©sum√©, m√©tadonn√©es)
    """
    try:
        # R√©cup√©rer les param√®tres de configuration LLM
        provider = st.session_state.llm_provider
        model = st.session_state.llm_model
        
        # Initialiser api_key et api_base avec des valeurs par d√©faut
        api_key = None
        api_base = None
        
        # Importer les cl√©s API depuis le fichier de configuration
        try:
            # V√©rifier si nous sommes sur Streamlit Cloud (les secrets sont accessibles)
            if hasattr(st, "secrets") and "api_keys" in st.secrets:
                api_key = st.secrets["api_keys"].get(provider)
                api_base = st.secrets.get("api_base_urls", {}).get(provider)
            
                # Si pas de token dans les secrets pour ce provider, afficher un avertissement
                if not api_key:
                    st.warning(f"Aucun token {provider} configur√© dans les secrets Streamlit.")
            else:
                # Si pas de secrets, essayer d'utiliser config.py local
                try:
                    from config import API_KEYS, API_BASE_URLS
                    api_key = API_KEYS.get(provider)
                    api_base = API_BASE_URLS.get(provider)
                except ImportError:
                    # Valeurs par d√©faut si le fichier n'existe pas
                    api_key = None
                    api_base = "https://models.inference.ai.azure.com" if provider == "github_inference" else None
                    st.warning("Fichier config.py non trouv√©. Les API n√©cessitant une authentification pourraient ne pas fonctionner.")
        except Exception as e:
            st.warning(f"Erreur lors de la r√©cup√©ration des cl√©s API: {str(e)}")
            # En dernier recours, utiliser des valeurs par d√©faut
            api_key = None
            api_base = "https://models.inference.ai.azure.com" if provider == "github_inference" else None
        
        # V√©rifier si GitHub Inference est choisi sans cl√© API
        if provider == "github_inference" and not api_key:
            return "Erreur: Aucune cl√© API GitHub Inference trouv√©e. Veuillez configurer une cl√© API dans config.py ou utiliser un autre fournisseur LLM comme Hugging Face.", {}
        
        # Cr√©er la configuration LLM
        config = LLMConfig(
            provider=provider,
            model_name=model,
            api_key=api_key,
            api_base=api_base,
            temperature=0.7,
            max_tokens=1500  # Augmenter pour les r√©sum√©s
        )
        
        # Si c'est Hugging Face, ajouter des param√®tres sp√©cifiques
        if provider == "huggingface":
            config.extra_params["use_api"] = True
        
        # Cr√©er le LLM
        llm = create_llm(config)
        
        # Cr√©er le retriever
        vector_store_path = "data/vector_store"
        retriever = create_default_retriever(
            store_path=vector_store_path,
            embedder_model="all-MiniLM-L6-v2",
            store_type="faiss",
            top_k=10
        )
        
        # Obtenir les noms des documents s√©lectionn√©s
        selected_doc_names = [st.session_state.documents[i].get("file_name") for i in doc_indices]
        
        # Cartographier les styles aux textes descriptifs
        style_map = {
            "informatif": "informatif et factuel",
            "analytique": "analytique avec des insights",
            "simplifi√©": "simplifi√© et accessible",
            "bullet_points": "sous forme de liste √† puces des points cl√©s"
        }
        style_description = style_map.get(style, "informatif et factuel")
        
        # APPROCHE ULTRA-SIMPLE: R√©cup√©rer directement le texte et construire un prompt
        # R√©cup√©rer les chunks pertinents
        all_chunks = []
        for doc_name in selected_doc_names:
            doc_chunks = retriever.retrieve(
                query=f"Contenu important de {doc_name}", 
                top_k=20,
                filter_metadata={"file_name": doc_name}
            )
            all_chunks.extend(doc_chunks)
        
        # Extraire le texte de tous les chunks
        document_text = ""
        for i, chunk in enumerate(all_chunks):
            document_text += f"--- Extrait {i+1} ---\n"
            document_text += chunk.get("text", "") + "\n\n"
        
        # Syst√®me prompt tr√®s explicite
        system_prompt = f"""Tu es un assistant sp√©cialis√© dans la cr√©ation de r√©sum√©s.
        Tu vas recevoir des extraits d'un ou plusieurs documents, et tu dois en faire un r√©sum√©.
        Le r√©sum√© doit √™tre {style_description} et faire {length}.
        Ne demande pas plus d'informations et ne mentionne pas que tu r√©sumes des extraits.
        Produis directement un r√©sum√© coh√©rent et bien structur√©."""
        
        # Prompt utilisateur direct
        user_prompt = f"""Voici des extraits de document(s) √† r√©sumer:

{document_text}

Cr√©e un r√©sum√© {style_description} de {length}. Le r√©sum√© doit √™tre clair, coh√©rent et couvrir les points principaux des extraits fournis."""
        
        # G√©n√©rer directement la r√©ponse avec le LLM
        llm_response = llm.generate(user_prompt, system_prompt=system_prompt)
        
        # Cr√©er les m√©tadonn√©es pour l'affichage
        metadata = {
            "length": length.split()[-2] if "environ" in length else length,
            "style": style,
            "num_docs": len(doc_indices),
            "doc_names": ", ".join(selected_doc_names),
            "sources": all_chunks[:5]  # Limiter le nombre de sources pour l'affichage
        }
        
        return llm_response.content, metadata
        
    except Exception as e:
        import traceback
        error_text = f"Erreur lors de la g√©n√©ration du r√©sum√©: {str(e)}\n\n{traceback.format_exc()}"
        st.error(error_text)
        return f"Erreur: {str(e)}", {}
    
   
def offer_download(summary_text, metadata):
    """
    Offre des options pour t√©l√©charger le r√©sum√©.
    
    Args:
        summary_text: Texte du r√©sum√©
        metadata: M√©tadonn√©es du r√©sum√©
    """
    # Pr√©parer le contenu du fichier
    buffer = io.StringIO()
    
    # Ajouter un en-t√™te
    buffer.write(f"# R√©sum√© g√©n√©r√© par AssistDoc\n\n")
    buffer.write(f"Style: {metadata.get('style', 'Standard')}\n")
    buffer.write(f"Longueur: {metadata.get('length', 'N/A')}\n")
    buffer.write(f"Documents: {metadata.get('num_docs', 'N/A')}\n\n")
    
    # Ajouter le r√©sum√©
    buffer.write("## R√©sum√©\n\n")
    buffer.write(summary_text)
    
    # Ajouter les sources si disponibles
    if "sources" in metadata and metadata["sources"]:
        buffer.write("\n\n## Sources\n\n")
        for i, source in enumerate(metadata["sources"], 1):
            doc_name = source.get("file_name", "Document inconnu")
            text_snippet = source.get("text", "")[:100] + "..."
            buffer.write(f"{i}. **{doc_name}**: {text_snippet}\n\n")
    
    # Ajouter un pied de page
    buffer.write("\n---\n")
    buffer.write("G√©n√©r√© avec AssistDoc - Assistant intelligent pour vos documents")
    
    # Proposer le t√©l√©chargement
    st.download_button(
        label="T√©l√©charger le r√©sum√©",
        data=buffer.getvalue(),
        file_name="resume_assistdoc.md",
        mime="text/markdown",
    )