"""
Page d'extraction d'informations de l'application AssistDoc.
Permet d'extraire des informations structur√©es des documents.
"""

import streamlit as st
import time
import json
import os
import re
from pathlib import Path
import io

# Import des modules de l'application
from ui.components.visualization import display_extraction_results, display_model_info
from src.vector_db.retriever import create_default_retriever
from src.llm.models import LLMConfig, LLMProvider, create_llm

def detect_streamlit_cloud():
    """D√©tecte si l'application s'ex√©cute sur Streamlit Cloud"""
    return os.path.exists("/mount/src")

def show_extraction_page():
    """
    Affiche la page d'extraction d'informations.
    """
    # Titre de la page
    st.markdown("<h1 class='main-title'>Extraction d'Informations üîç</h1>", unsafe_allow_html=True)
    
    # V√©rifier si des documents sont charg√©s
    if not st.session_state.get("documents", []) or not st.session_state.get("vector_store_initialized", False):
        st.warning("Aucun document charg√©. Veuillez d'abord charger et indexer des documents dans la barre lat√©rale.")
        return
    
    # Informations sur le mod√®le utilis√©
    display_model_info()
    
    # Panneau de configuration de l'extraction
    with st.expander("Options d'extraction", expanded=True):
        # S√©lection du document √† analyser
        doc_options = [doc.get("file_name", f"Document {i+1}") for i, doc in enumerate(st.session_state.documents)]
        
        selected_doc = st.selectbox(
            "Document √† analyser",
            options=doc_options
        )
        
        # Obtenir l'index du document s√©lectionn√©
        selected_doc_index = doc_options.index(selected_doc)
        selected_doc_id = st.session_state.documents[selected_doc_index].get("file_name")
        
        # √âl√©ments √† extraire
        st.subheader("√âl√©ments √† extraire")
        
        # Types d'extraction pr√©d√©finis
        extraction_types = {
            "custom": "Personnalis√©",
            "metadata": "M√©tadonn√©es du document",
            "contacts": "Informations de contact",
            "dates": "Dates et √©v√©nements",
            "entities": "Entit√©s nomm√©es",
            "key_concepts": "Concepts cl√©s"
        }
        
        extraction_type = st.selectbox(
            "Type d'extraction",
            options=list(extraction_types.keys()),
            format_func=lambda x: extraction_types[x],
            index=1  # Option par d√©faut: m√©tadonn√©es
        )
        
        # D√©finir les √©l√©ments √† extraire selon le type s√©lectionn√©
        predefined_items = {
            "metadata": ["titre", "auteur", "date", "organisation", "mots_cl√©s", "sujet_principal"],
            "contacts": ["noms", "emails", "t√©l√©phones", "adresses", "organisations", "r√¥les"],
            "dates": ["dates_mentionn√©es", "√©v√©nements", "√©ch√©ances", "p√©riodes_importantes"],
            "entities": ["personnes", "organisations", "lieux", "produits", "technologies"],
            "key_concepts": ["th√®mes_principaux", "concepts_cl√©s", "terminologie", "id√©es_principales", "conclusions"]
        }
        
        # Si type personnalis√©, permettre √† l'utilisateur de d√©finir ses propres √©l√©ments
        if extraction_type == "custom":
            custom_items = st.text_area(
                "√âl√©ments √† extraire (un par ligne)",
                value="titre\nauteur\ndate\nmots_cl√©s",
                height=150,
                help="Entrez un √©l√©ment √† extraire par ligne"
            )
            
            # Convertir en liste
            items_to_extract = [item.strip() for item in custom_items.split('\n') if item.strip()]
        else:
            # Utiliser les √©l√©ments pr√©d√©finis
            items_to_extract = predefined_items.get(extraction_type, [])
            
            # Afficher les √©l√©ments qui seront extraits
            st.write("√âl√©ments qui seront extraits:")
            for item in items_to_extract:
                st.write(f"- {item}")
        
        # Format de sortie
        col1, col2 = st.columns(2)
        
        with col1:
            format_options = ["JSON", "Tableau", "Texte"]
            output_format = st.selectbox("Format de sortie", options=format_options, index=0)
        
        with col2:
            # Option pour inclure d'autres documents
            if len(st.session_state.documents) > 1:
                multi_doc = st.checkbox("Inclure d'autres documents", value=False)
                
                if multi_doc:
                    additional_docs = st.multiselect(
                        "Documents additionnels",
                        options=[doc for doc in doc_options if doc != selected_doc],
                        default=[],
                        help="S√©lectionnez des documents suppl√©mentaires √† analyser"
                    )
                    
                    # Ajouter le document principal aux documents s√©lectionn√©s
                    selected_docs = [selected_doc] + additional_docs
                    
                    # Filtrer les documents s√©lectionn√©s
                    selected_doc_indices = [i for i, name in enumerate(doc_options) if name in selected_docs]
                else:
                    # Si un seul document est s√©lectionn√©
                    selected_doc_indices = [selected_doc_index]
            else:
                # Si un seul document est charg√©, le s√©lectionner automatiquement
                selected_doc_indices = [0]
    
    # Bouton pour lancer l'extraction
    if st.button("Extraire les informations", type="primary", use_container_width=True):
        # V√©rifier qu'au moins un document est s√©lectionn√©
        if not selected_doc_indices:
            st.error("Veuillez s√©lectionner au moins un document √† analyser")
            return
        
        # V√©rifier qu'au moins un √©l√©ment est √† extraire
        if not items_to_extract:
            st.error("Veuillez sp√©cifier au moins un √©l√©ment √† extraire")
            return
        
        # Lancer l'extraction
        with st.spinner("Extraction des informations en cours..."):
            extraction_result = extract_information(
                items_to_extract,
                output_format,
                selected_doc_indices
            )
        
        # Afficher les r√©sultats
        if extraction_result:
            st.subheader("R√©sultats de l'extraction")
            
            # Format d'affichage
            display_format = "json" if output_format == "JSON" else "table" if output_format == "Tableau" else "text"
            display_extraction_results(extraction_result, display_format)
            
            # Permettre le t√©l√©chargement des r√©sultats
            offer_download(extraction_result, output_format, items_to_extract)

def extract_information(items_to_extract, output_format, doc_indices):
    """
    Extrait des informations des documents s√©lectionn√©s.
    
    Args:
        items_to_extract: Liste des √©l√©ments √† extraire
        output_format: Format de sortie souhait√©
        doc_indices: Indices des documents √† analyser
        
    Returns:
        R√©sultat de l'extraction (structure JSON ou texte)
    """
    try:
        # R√©cup√©rer les param√®tres de configuration LLM
        provider = st.session_state.llm_provider
        model = st.session_state.llm_model
        
        # Initialiser api_key et api_base avec des valeurs par d√©faut
        api_key = None
        api_base = None
        
        # Importer les cl√©s API depuis le fichier de configuration
        try:
            # V√©rifier si nous sommes sur Streamlit Cloud (les secrets sont accessibles)
            if hasattr(st, "secrets") and "api_keys" in st.secrets:
                api_key = st.secrets["api_keys"].get(provider)
                api_base = st.secrets.get("api_base_urls", {}).get(provider)
            
                # Si pas de token dans les secrets pour ce provider, afficher un avertissement
                if not api_key:
                    st.warning(f"Aucun token {provider} configur√© dans les secrets Streamlit.")
            else:
                # Si pas de secrets, essayer d'utiliser config.py local
                try:
                    from config import API_KEYS, API_BASE_URLS
                    api_key = API_KEYS.get(provider)
                    api_base = API_BASE_URLS.get(provider)
                except ImportError:
                    # Valeurs par d√©faut si le fichier n'existe pas
                    api_key = None
                    api_base = "https://models.inference.ai.azure.com" if provider == "github_inference" else None
                    st.warning("Fichier config.py non trouv√©. Les API n√©cessitant une authentification pourraient ne pas fonctionner.")
        except Exception as e:
            st.warning(f"Erreur lors de la r√©cup√©ration des cl√©s API: {str(e)}")
            # En dernier recours, utiliser des valeurs par d√©faut
            api_key = None
            api_base = "https://models.inference.ai.azure.com" if provider == "github_inference" else None
        
        # V√©rifier si GitHub Inference est choisi sans cl√© API
        if provider == "github_inference" and not api_key:
            return "Erreur: Aucune cl√© API GitHub Inference trouv√©e. Veuillez configurer une cl√© API ou utiliser un autre fournisseur LLM comme Hugging Face."
        
        # Cr√©er la configuration LLM
        config = LLMConfig(
            provider=provider,
            model_name=model,
            api_key=api_key,
            api_base=api_base,
            temperature=0.3,  # R√©duire la temp√©rature pour l'extraction (plus pr√©cis)
            max_tokens=1500
        )
        
        # Si c'est Hugging Face, ajouter des param√®tres sp√©cifiques
        if provider == "huggingface":
            config.extra_params["use_api"] = True
        
        # Cr√©er le LLM
        llm = create_llm(config)
        
        # Cr√©er le retriever - Toujours utiliser FAISS sur Streamlit Cloud
        vector_store_path = "data/vector_store"
        store_type = "faiss"  # Utiliser FAISS pour assurer la compatibilit√©
        retriever = create_default_retriever(
            store_path=vector_store_path,
            embedder_model="all-MiniLM-L6-v2",
            store_type=store_type,
            top_k=8
        )
        
        # Obtenir les noms des documents s√©lectionn√©s
        selected_doc_names = [st.session_state.documents[i].get("file_name") for i in doc_indices]
        
        # APPROCHE DIRECTE: r√©cup√©rer les chunks, les filtrer, puis g√©n√©rer la r√©ponse
        all_chunks = []
        
        # Pour chaque document, r√©cup√©rer des chunks pertinents
        for doc_name in selected_doc_names:
            # R√©cup√©rer tous les chunks disponibles pour ce document
            chunks = retriever.retrieve(
                query=f"informations importantes sur {doc_name}",
                top_k=20
            )
            
            # Filtrer pour ce document sp√©cifique
            doc_chunks = [chunk for chunk in chunks if chunk.get("file_name") == doc_name]
            
            # Si aucun chunk trouv√©, essayer d'acc√©der au texte brut du document
            if not doc_chunks:
                for i, doc in enumerate(st.session_state.documents):
                    if doc.get("file_name") == doc_name and "full_text" in doc:
                        # Diviser le texte en chunks
                        text = doc["full_text"]
                        chunk_size = 1000
                        
                        for j in range(0, len(text), chunk_size):
                            end = min(j + chunk_size, len(text))
                            doc_chunks.append({
                                "text": text[j:end],
                                "file_name": doc_name,
                                "score": 0.5
                            })
                        
                        # Limiter le nombre de chunks
                        if doc_chunks:
                            doc_chunks = doc_chunks[:5]
                            break
            
            all_chunks.extend(doc_chunks)
        
        # Limiter le nombre total de chunks
        all_chunks = all_chunks[:15]
        
        # Formater les chunks en texte
        context_text = ""
        for i, chunk in enumerate(all_chunks):
            doc_name = chunk.get("file_name", "Document inconnu")
            context_text += f"[EXTRAIT {i+1} de {doc_name}]\n{chunk.get('text', '')}\n\n"
        
        # Formater les √©l√©ments √† extraire
        items_str = "\n".join([f"- {item}" for item in items_to_extract])
        
        # Syst√®me prompt tr√®s directif
        system_prompt = f"""Tu es un assistant sp√©cialis√© dans l'extraction d'informations pr√©cises √† partir de documents.
        Tu dois extraire UNIQUEMENT les informations demand√©es dans le format sp√©cifi√©.
        Si une information n'est pas trouv√©e, indique-le par "Non trouv√©" ou "N/A"."""
        
        # Construire le prompt utilisateur
        user_prompt = f"""Extrais les informations suivantes des documents fournis:

{items_str}

EXTRAITS DES DOCUMENTS:
{context_text}

Format de sortie: {output_format}
"""
        
        if output_format == "JSON":
            user_prompt += """
R√©ponds UNIQUEMENT avec un objet JSON valide contenant les informations extraites.
Exemple de format attendu:
{
  "titre": "Titre du document",
  "auteur": "Nom de l'auteur",
  "date": "Date trouv√©e",
  ...
}
"""
        elif output_format == "Tableau":
            user_prompt += """
R√©ponds avec les informations format√©es de mani√®re structur√©e pour un tableau.
Chaque √©l√©ment doit avoir une valeur claire, m√™me si c'est "Non trouv√©" quand l'information n'est pas disponible.
Je convertis ta r√©ponse en format tabulaire, donc la structure doit √™tre nette et pr√©cise.
"""
        
        # G√©n√©rer la r√©ponse
        response = llm.generate(user_prompt, system_prompt=system_prompt)
        
        # Traiter la r√©ponse selon le format demand√©
        content = response.content
        
        # Si format JSON, essayer d'extraire et parser le JSON
        if output_format == "JSON":
            # Chercher un objet JSON valide dans la r√©ponse
            import re
            
            # Chercher un texte entre accolades
            json_match = re.search(r'({[\s\S]*})', content)
            if json_match:
                json_str = json_match.group(1)
                try:
                    # Tenter de parser le JSON
                    parsed_json = json.loads(json_str)
                    return parsed_json
                except json.JSONDecodeError:
                    pass
        
        # Si format Tableau et pas d√©j√† JSON, essayer de le convertir en structure tabulaire
        elif output_format == "Tableau":
            try:
                import re
                # D'abord essayer de voir si le texte contient du JSON
                json_match = re.search(r'({[\s\S]*})', content)
                if json_match:
                    try:
                        parsed_json = json.loads(json_match.group(1))
                        return parsed_json
                    except:
                        pass
                
                # Sinon, essayer de parser le texte en format cl√©-valeur
                result_dict = {}
                
                # Chercher les paires cl√©-valeur (format "cl√©: valeur" ou "cl√© - valeur")
                lines = content.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    if not line or line.startswith('|') or line.startswith('-'):  # Ignorer les lignes de s√©paration
                        continue
                    
                    # Essayer diff√©rents formats de s√©paration cl√©-valeur
                    separators = [':', '-', '=']
                    for sep in separators:
                        if sep in line:
                            parts = line.split(sep, 1)
                            if len(parts) == 2:
                                key = parts[0].strip().rstrip(':.-').strip()
                                value = parts[1].strip()
                                result_dict[key] = value
                                break
                
                # Si des donn√©es ont √©t√© extraites, retourner le dictionnaire
                if result_dict:
                    return result_dict
                
                # Derni√®re tentative: essayer de parser comme tableau markdown ou CSV
                if "|" in content:
                    import pandas as pd
                    import io
                    
                    # Nettoyer le contenu pour le format markdown
                    lines = [line.strip() for line in content.strip().split('\n') if line.strip()]
                    
                    # Trouver les lignes qui semblent √™tre un en-t√™te et des donn√©es
                    header_line = None
                    data_lines = []
                    
                    for i, line in enumerate(lines):
                        if "|" in line:
                            if not header_line:
                                header_line = line
                            else:
                                # Ignorer les lignes de s√©paration (avec des tirets)
                                if not all(c == '-' or c == '|' or c.isspace() for c in line):
                                    data_lines.append(line)
                    
                    if header_line and data_lines:
                        # Parser l'en-t√™te
                        headers = [h.strip() for h in header_line.split('|')]
                        headers = [h for h in headers if h]  # Supprimer les cellules vides
                        
                        # Parser les donn√©es
                        data = {}
                        for header in headers:
                            data[header] = []
                        
                        for line in data_lines:
                            cells = [c.strip() for c in line.split('|')]
                            cells = [c for c in cells if c]  # Supprimer les cellules vides
                            
                            for i, cell in enumerate(cells):
                                if i < len(headers):
                                    data[headers[i]].append(cell)
                        
                        # V√©rifier qu'on a des donn√©es
                        if any(data.values()):
                            # Convertir en dictionnaire simple si une seule ligne
                            if all(len(v) == 1 for v in data.values()):
                                return {k: v[0] for k, v in data.items()}
                            else:
                                return data
            except Exception as e:
                st.warning(f"Erreur lors de la conversion en tableau: {str(e)}")
        
        # Retourner le texte brut si le parsing a √©chou√©
        return content
        
    except Exception as e:
        import traceback
        st.error(f"Erreur lors de l'extraction des informations: {str(e)}")
        st.error(traceback.format_exc())
        return f"Erreur lors de l'extraction: {str(e)}"

def offer_download(extraction_result, format_type, items_extracted):
    """
    Offre des options pour t√©l√©charger les r√©sultats d'extraction.
    
    Args:
        extraction_result: R√©sultat de l'extraction
        format_type: Format souhait√© pour le t√©l√©chargement
        items_extracted: Liste des √©l√©ments extraits
    """
    try:
        # Pr√©parer le contenu du fichier selon le format
        if format_type == "JSON":
            # Si c'est d√©j√† un dictionnaire
            if isinstance(extraction_result, dict):
                content = json.dumps(extraction_result, indent=2, ensure_ascii=False)
                mime_type = "application/json"
                file_extension = "json"
            else:
                # Essayer de convertir en JSON
                try:
                    from ui.components.visualization import extract_json_from_text
                    json_text = extract_json_from_text(extraction_result)
                    content = json_text
                    mime_type = "application/json"
                    file_extension = "json"
                except:
                    content = extraction_result
                    mime_type = "text/plain"
                    file_extension = "txt"
        elif format_type == "Tableau":
            # Cr√©er un CSV propre √† partir des donn√©es
            if isinstance(extraction_result, dict):
                # En-t√™te CSV
                content = ",".join([f'"{k}"' for k in extraction_result.keys()]) + "\n"
                
                # Ligne de donn√©es
                values = []
                for v in extraction_result.values():
                    # G√©rer les valeurs complexes
                    if isinstance(v, (list, dict)):
                        v = json.dumps(v, ensure_ascii=False)
                    # √âchapper les guillemets
                    if isinstance(v, str):
                        v = v.replace('"', '""')
                    values.append(f'"{v}"')
                
                content += ",".join(values)
                mime_type = "text/csv"
                file_extension = "csv"
            else:
                content = extraction_result
                mime_type = "text/plain"
                file_extension = "txt"
        else:
            # Format texte par d√©faut
            content = extraction_result
            mime_type = "text/plain"
            file_extension = "txt"
        
        # Proposer le t√©l√©chargement
        st.download_button(
            label=f"T√©l√©charger ({file_extension.upper()})",
            data=content,
            file_name=f"extraction_assistdoc.{file_extension}",
            mime=mime_type,
        )
    except Exception as e:
        st.error(f"Erreur lors de la pr√©paration du t√©l√©chargement: {str(e)}")